{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEN' 'STL' 'DAL' 'DET' 'JAX' 'NOR' 'BUF' 'NYJ' 'CHI' 'IND' 'PIT' 'SFO'\n",
      " 'CAR' 'CLE' 'SDG' 'WAS' 'NWE' 'TAM' 'KAN' 'HOU' 'ATL' 'NYG' 'SEA' 'PHI'\n",
      " 'ARI' 'BAL' 'OAK' 'GNB' 'CIN' 'MIN' 'MIA' 'TEN' 'LAR' 'LAC']\n"
     ]
    }
   ],
   "source": [
    "print(X['Home'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently tests two baseline models for the 2017 season.  Will update with more data as it is acquired.\n",
    "\n",
    "First baseline is an average of the pointspread predictions from various casinos and betting outlets. This does not require training, though we could train a ridge regression model and expect a similar result (as we'd expect the betting outlets to be relatively equally predictive on average as well as fairly correlated, so their weights should be very close under L2-regularized linear regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import data\n",
    "odds = pd.read_csv(\"base_data/nfl_odds_2017.csv\")\n",
    "game_stats = pd.read_csv(\"base_data/nflstats2017.csv\")\n",
    "game_stats.rename(index = str, columns = {'HPS': 'APS', 'HPSY': 'APSY', 'HPS.1':'HPS', 'HPSY.1':'HPSY'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.30069278331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: Mean of empty slice\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#Formatting to create key and filter\n",
    "odds[\"datestring\"] = pd.to_datetime(odds['date']).dt.strftime('%Y%m%d')\n",
    "odds[\"Spread_key\"] = odds['datestring'] + odds['Home']\n",
    "odds['Spread_val'] = odds['HomeScore'] - odds['AwayScore']\n",
    "spreads = odds.filter(regex = \"Spread_\")\n",
    "\n",
    "#Compute square loss over 2017 season\n",
    "X = spreads.values[:,:-2].astype('float32')\n",
    "y = spreads['Spread_val'].values.astype('float32')\n",
    "scores = np.nanmean(X, axis = 1)\n",
    "residual = (scores - y)**2\n",
    "loss = np.nanmean(residual)/len(y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second baseline is a regression tree model that will use basic categorical features (Home/Away team, start time, home/away team stats) to predict the outcome.  For current purposes, will train on first 8 weeks of the season and test on the rest (this will change with more data).  This requires a lot more preprocessing, as we need to aggregate average statistics at that point in the season for each team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "season_data = []\n",
    "spread_data = []\n",
    "\n",
    "for y in range(2013,2018):\n",
    "    temp = pd.read_csv('processed_data/' + str(y) + 'processed.csv')\n",
    "    spread_data.append(temp['Spread'])\n",
    "    season_data.append(temp.drop(['Spread','Key','Datetime'], axis = 1))\n",
    "\n",
    "X = pd.concat(season_data)\n",
    "X_cat = pd.get_dummies(X,['Home', 'Away'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = pd.concat(spread_data[0:3])\n",
    "y_val = spread_data[3]\n",
    "y_test = spread_data[4]\n",
    "\n",
    "train_size = len(y_train)\n",
    "val_size = len(y_val)\n",
    "test_size = len(y_test)\n",
    "\n",
    "X_train = X_cat[0:train_size]\n",
    "X_val = X_cat[train_size:(train_size + val_size)]\n",
    "X_test = X_cat[(train_size + val_size): ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('l2', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'l2__alpha': [750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804,...830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "pipe = Pipeline(steps = [('scaler', StandardScaler()),\n",
    "                        ('l2',Ridge()) ])\n",
    "\n",
    "lam_reg = [x + 750 for x in range(0,100)]\n",
    "\n",
    "grid = GridSearchCV(pipe,\n",
    "                    param_grid={'l2__alpha': lam_reg},\n",
    "                    cv=2)\n",
    "\n",
    "\n",
    "grid.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149.400499748\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('l2', Ridge(alpha=786, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001))])\n"
     ]
    }
   ],
   "source": [
    "print(sum((grid.predict(X_val) - y_val)**2)/val_size)\n",
    "print(grid.best_estimator_)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.6623335649\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.98722768943364"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((grid.predict(X_test).transpose() - y_test)**2)/test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_grid = GridSearchCV(DecisionTreeRegressor,\n",
    "                    param_grid={'l2__max_dept': [None, 10, 100, 3, 25]},\n",
    "                    cv=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sum((tree_grid.predict(X_val) - y_val)**2)/val_size)\n",
    "print(tree_grid.best_estimator_)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
